use quote::quote;

pub(crate) fn include_lexer(mut lex_parser: crate::LexParser) -> proc_macro::TokenStream {
    let (token_names, token_type_names, token_types) = lex_parser.parse_token_decs();
    let (regex_names, regex_vals) = lex_parser.parse_regular_defs();
    let (match_regex, ret_tokens) = lex_parser.parse_match_rules();

    // let mut token_enums = crate::gen_token_enums(token_names, token_type_names, token_types);
    let token_match_arms = token_match_arms(token_names, token_type_names, token_types);

    let lexer_code: proc_macro::TokenStream = quote! {

        pub struct Lexer<const N: usize> {
            dfa: wist_lex::dfa::DFA<N>,
            current_state: [wist_lex::regex::Regex; N],
            input_buffer: String,
            token_kinds: ::std::collections::HashMap<wist_lex::regex::Regex, LexTokenKind>,
            emitting_regex: Option<(usize, String)>,
        }

        impl<const N: usize> Lexer<N> {
            pub fn new() -> Result<Self, wist_lex::Error> {
                use ::std::collections::{HashSet, HashMap};
                use wist_lex::regex::{Regex, Set};

                let regular_defs = vec![#((#regex_names, #regex_vals)),*];

                let mut parser = wist_lex::RegexParser::new();
                let mut definitions: HashMap<String, Regex> = HashMap::new();
                for (name, regex) in regular_defs {
                    definitions.insert(
                        name.to_string(),
                        Regex::from_ast(parser.parse(&regex)?, &definitions),
                    );
                    parser.reset();
                }

                let match_rules = vec![#(#match_regex),*];
                let mut regexes: [Regex; N] = (0..N)
                    .map(|_| wist_lex::EMPTY_REGEX.clone())
                    .collect::<Vec<Regex>>()
                    .try_into()
                    .unwrap();
                for (i, regex) in match_rules.iter().enumerate() {
                    regexes[i] = Regex::from_ast(parser.parse(&regex)?, &definitions);
                    parser.reset();
                }

                let ret_tokens = vec![#(LexTokenKind::#ret_tokens),*];
                let token_kinds: HashMap<Regex, LexTokenKind> = regexes.clone().into_iter().zip(ret_tokens).collect();

                let dfa = wist_lex::dfa::DFA::<N>::from_regexes(regexes);
                Ok(Lexer {
                    emitting_regex: None,
                    input_buffer: "".to_string(),
                    current_state: dfa.start_state.clone(),
                    token_kinds,
                    dfa,
                })
            }

            fn lex_file(&mut self, path: String) -> ::std::io::Result<Vec<LexToken>> {
                let file = ::std::fs::File::open(path)?;
                use ::std::io::prelude::*;
                let mut lines = ::std::io::BufReader::new(file).lines();

                let mut output_tokens = Vec::new();

                for line in lines {
                    let mut chars: Vec<char> = line?.chars().collect();
                    while !chars.is_empty() {
                        let c = chars.remove(0);
                        self.input_buffer.push(c);
                        match self.step(c) {
                            None => (),
                            Some(token) => {
                                if token.kind != LexTokenKind::_SKIP {
                                    output_tokens.push(token);
                                }

                                self.current_state = self.dfa.start_state.clone();

                                let clone = self.emitting_regex.clone();
                                let truncated = self.input_buffer
                                    [clone.unwrap().1.len()..]
                                    .chars();
                                for (i, c) in truncated.enumerate() {
                                    chars.insert(i, c);
                                }
                                
                                self.emitting_regex = None;
                                self.input_buffer = String::new();
                            }
                        }
                    }

                }

                let token = self.produce_token();
                if token.kind != LexTokenKind::_SKIP {
                    output_tokens.push(token);
                }

                Ok(output_tokens)
            }

            fn step(&mut self, c: char) -> Option<LexToken> {
                match self.dfa.transitions.get(&self.current_state) {
                    Some(transitions) => {
                        for (set, next_state) in transitions {
                            if set.contains(c) {
                                self.current_state = next_state.clone();
                                self.update_emitting_regex();
                                return None;
                            }
                        }
                        panic!("missing transition")
                    }
                    None => Some(self.produce_token())
                }
            }

            fn update_emitting_regex(&mut self) {
                for (i, regex) in self.current_state.iter().enumerate() {
                    if regex.nullable() {
                        self.emitting_regex = Some((i, self.input_buffer.clone())); 
                        break;
                    }
                }
            }

            fn produce_token(&mut self) -> LexToken {
                let token = match &self.emitting_regex {
                    None => panic!("Need to emit token"),
                    Some((i, value)) => {
                        let matching_regex = &self.dfa.start_state[*i];
                        let token_kind = self.token_kinds.get(matching_regex)
                            .unwrap_or_else(|| panic!("Regex has no matching token"));
                        println!("{:?}", value);
                        Self::create_token(*token_kind, value.clone())
                    },
                };

                token
            }
            fn create_token(kind: LexTokenKind, value: String) -> LexToken {
                use LexTokenKind::*;
                match kind {
                    _SKIP => {
                        LexToken {
                            kind,
                            val: LexTokenValue::None,
                        }
                    }
                    #token_match_arms,
                }
            }
        }
    }.into();

    let mut token_enums: proc_macro::TokenStream = quote!{}.into();
    token_enums.extend(lexer_code.into_iter());
    token_enums
}

fn token_match_arms(token_names: Vec<syn::Ident>, token_type_names: Vec<Option<syn::Ident>>, token_types: Vec<Option<syn::Type>>) -> proc_macro2::TokenStream {
    let token_vals: Vec<_> = token_type_names.iter().zip(&token_types).map(|(t_v, t)| crate::token_type_to_val(t_v, t)).collect();
    
    quote!{
        #(#token_names => LexToken {
            kind: kind,
            val: LexTokenValue::#token_vals,
        }),*
    }
}
